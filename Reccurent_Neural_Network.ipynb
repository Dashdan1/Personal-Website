{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielgconti/Personal-Website/blob/main/Reccurent_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Download the dataset"
      ],
      "metadata": {
        "id": "wFqXcqKRcq7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our goal today is to train a sentiment analysis neural network, which can determine whether a piece of text is positive (happy) or negative (unhappy). To do this, we will use the [Amazon Reviews for Sentiment Analysis](https://www.kaggle.com/datasets/bittlingmayer/amazonreviews) dataset, which contains many 1-2 star reviews and many 4-5 star reviews."
      ],
      "metadata": {
        "id": "oVKmdTnFOF11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1A: Download Data from Kaggle"
      ],
      "metadata": {
        "id": "pGaPK5eMPX8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown We are downloading the dataset from Kaggle. To do this using code, we need access to the Kaggle API, which you can get by downloading an API key from the Kaggle website. Download the key by going to **Kaggle → Account → Create New API Token**, and then run this cell and upload the `kaggle.json` file you created.\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "SbPoAWdYcxez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you've uploaded the `kaggle.json` file containing your API key, run the following code to install and prepare the Kaggle API python package:"
      ],
      "metadata": {
        "id": "gikL1wKVOsym"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F8cfF-ccnVA"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, run this command to download and unzip the dataset:"
      ],
      "metadata": {
        "id": "E-Oxi6fjOxUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d bittlingmayer/amazonreviews\n",
        "!unzip amazonreviews.zip -d data"
      ],
      "metadata": {
        "id": "yIbCg1lpc5BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, read the text files as python variables. This will take about 2 minutes to run:"
      ],
      "metadata": {
        "id": "3KXcW7LdO3rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bz2\n",
        "train_file_all = bz2.BZ2File(\"./data/train.ft.txt.bz2\").readlines()\n",
        "test_file_all = bz2.BZ2File(\"./data/test.ft.txt.bz2\").readlines()"
      ],
      "metadata": {
        "id": "zoC7H9SEc_Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is absolutely massive, which could be great if we had a supercomputer, but to keep this managable for our purposes today, let's take a smaller sample from the full dataset (by choosing a total of 200,000 random reviews out of the millions of reviews in the dataset):"
      ],
      "metadata": {
        "id": "7HcHQMqHO8Gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Take a smaller sample from the dataset to work with\n",
        "num_train = 160000\n",
        "num_test = 40000\n",
        "\n",
        "train_file = [x.decode(\"utf-8\") for x in random.sample(train_file_all, num_train)]\n",
        "test_file = [x.decode(\"utf-8\") for x in random.sample(test_file_all, num_test)]"
      ],
      "metadata": {
        "id": "KkS0kbtzdBYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1B: Split into sentences & labels"
      ],
      "metadata": {
        "id": "VFVY1rkaPUil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The variable `train_file` is an array of strings, where each string is a line of text from the training data. (Similarly, `test_file` is an array of strings from the testing data.)\n",
        "\n",
        "**Your job: Print out the first item from the `train_file` array to see what each review string looks like.**"
      ],
      "metadata": {
        "id": "DWST5SG0PygS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ??? TODO: Print the first item of train_file\n",
        "print(train_file[0])"
      ],
      "metadata": {
        "id": "vGMk_uxyPNYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that this string begins with either `__label__0` or `__label__1`, and then a space, and then the review text. `__label__0` means that it's a positive review (good, 4-5 stars) and `__label__1` means that it's a negative review (bad, 1-2 stars). Hopefully your example above is correctly labelled.\n",
        "\n",
        "What we really want is to split `train_file` into two different variables, one array containing strings of just the review text, and one array containing just the numbers 0 or 1 corresponding to whether each review is negative or positive. (We're going to swap the labelling so that 0 means negative and 1 means positive, because that feels much more intuitive.)"
      ],
      "metadata": {
        "id": "VcuBQkuFQWq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The data set gives us two labels, which we can interpret as a positive or negative review\n",
        "\n",
        "train_labels = [0 if x.split(\" \")[0] == \"__label__1\" else 1 for x in train_file]\n",
        "train_sentences = [x.split(\" \", 1)[1][:-1].lower() for x in train_file]\n",
        "\n",
        "test_labels = [0 if x.split(\" \")[0] == \"__label__1\" else 1 for x in test_file]\n",
        "test_sentences = [x.split(\" \", 1)[1][:-1].lower() for x in test_file]"
      ],
      "metadata": {
        "id": "u_EUrDuPdDHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Print out the first five elements of `train_labels` and the first five elements of `train_sentences` to make sure the sentences and their label numbers seem correct:**"
      ],
      "metadata": {
        "id": "NRYsp_ruRaSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ??? TODO: Print the first five elements of train_labels and the first five\n",
        "# elements of train_sentences to make sure the sentences and their labels\n",
        "# look good and make sense.\n",
        "for x in range(5):\n",
        "  print(train_labels[x])\n",
        "  print(train_sentences[x])"
      ],
      "metadata": {
        "id": "DW1Xwtrvdy2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2"
      ],
      "metadata": {
        "id": "Iq9qFHVweECq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since RNNs operate on *sequences* of data, we need to split our paragraph-long review strings into sequences of words (or, actually, sequences of **tokens**, which are slightly smaller chunks than words)."
      ],
      "metadata": {
        "id": "WbrWpswGTG5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2A: Tokenize the Paragraphs"
      ],
      "metadata": {
        "id": "ltCrhTNdeLrf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code imports and prepares a library called `nltk` which will tokenize text for us automatically:"
      ],
      "metadata": {
        "id": "XOM-6etXTBqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "NBF0dS0veIlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Give it a try!** Call the function `nltk.word_tokenize()` and pass it a string with some kind of sentence that you want to tokenize."
      ],
      "metadata": {
        "id": "6ldGsVHeTSrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ??? TODO: Call nltk.word_tokenize() and pass in a sentence string to try tokenizing\n",
        "nltk.word_tokenize(\"What's up this is a little string I made up to demonstrate tokenization\")"
      ],
      "metadata": {
        "id": "7o_MPJ1IeP_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright, so we can tokenize sentences. The variables `train_sentences` and `test_sentences` are each lists of strings. The following code converts `train_sentences` into a new list, `train_tokens`, which is actually a list of lists: Each sentences has been converted to a list of tokens.\n",
        "\n",
        "**Edit the code below so that in addition to converting `train_sentences` into `train_tokens`, it also converts `test_sentences` into `test_tokens`.** (This code will take about 2 minutes to run.)"
      ],
      "metadata": {
        "id": "o0vzWzbaTYWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokens = [nltk.word_tokenize(sentence) for sentence in train_sentences]\n",
        "test_tokens = [nltk.word_tokenize(sentence) for sentence in test_sentences]"
      ],
      "metadata": {
        "id": "sUFAzZbJe-Rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print out an example sentence and tokenized version from the training set and from the testing set:"
      ],
      "metadata": {
        "id": "qUYK6Sh6T1Bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First sentence from training set:\")\n",
        "print(train_sentences[0])\n",
        "print(train_tokens[0])\n",
        "\n",
        "print(\"\\nFirst sentence from test set:\")\n",
        "print(test_sentences[0])\n",
        "print(test_tokens[0])"
      ],
      "metadata": {
        "id": "8KwwiwxRfQCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2B: Convert Tokens to Numbers"
      ],
      "metadata": {
        "id": "kM5g3qQCjXxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computers really like numbers, so we want to convert our tokens (words, basically) into numbers. To do this, we'll create a dictionary of all the tokens in our dataset, and assign a number to each token.\n",
        "\n",
        "Let's start by counting up how many times each different token appears:"
      ],
      "metadata": {
        "id": "ws_mQlc_DOtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Count the number of times each word shows up, so that we can calculate the size of our vocabulary\n",
        "frequencies = Counter()\n",
        "\n",
        "for i, tokens in enumerate(train_tokens):\n",
        "  frequencies.update([token.lower() for token in tokens])"
      ],
      "metadata": {
        "id": "R9WZKvLoeoiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In theory, `frequencies` should now be a counter that can tell us how many times any given word appears. Try running the following code to check how many times the word \"good\" appears in the dataset. **Then, try checking the count for a different word.**"
      ],
      "metadata": {
        "id": "zSFWkdW5EHmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ??? TODO: Try editing the line below to check the frequency of a different word\n",
        "print(frequencies[\"bad\"]) # Check number of times word appeared"
      ],
      "metadata": {
        "id": "JsV1lXNHgGCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to assign a number to each token. It's common practice to assign smaller numbers (1, 2, 3...) to the most common words and larger numbers (100, 1000...) to less common words. This code creates a sorted list of tokens (most to least common):"
      ],
      "metadata": {
        "id": "vb7nPmvTEiQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [\"_PAD\", \"_UNK\"] + sorted(frequencies, key=frequencies.get, reverse=True)\n",
        "print(tokens[:100])"
      ],
      "metadata": {
        "id": "Bu9Z70Feg-JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that we've also added to special tokens, \"_PAD\" and \"_UNK\" which stand for \"padding\" and \"unknown\". We'll use these in cases where we get an unrecognized token (unknown) or need to fill extra space (padding).\n",
        "\n",
        "Finally, let's assign a number to each token in the list:"
      ],
      "metadata": {
        "id": "6Eil0dURFb60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token2num = {n: i for i, n in enumerate(tokens)}\n",
        "token2num"
      ],
      "metadata": {
        "id": "QK74TqMgh-RA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can convert the training and testing data, `train_tokens` and `test_tokens`, from lists of token strings (which they currently are) into lists of numbers, which we'll call `train_x` and `train_y`. (We'll also copy `train_labels` and `test_labels` into `train_y` and `test_y` to be consistent with naming.)\n",
        "\n",
        "The following code creates number versions `train_x` and `train_y`. **Write some additional code to generate `test_x` and `test_y`.**"
      ],
      "metadata": {
        "id": "ccFT2rVdFrLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def to_numbers(tokens):\n",
        "  return [token2num[token] if token in token2num else 1 for token in tokens]\n",
        "\n",
        "train_x = [to_numbers(tokens) for tokens in train_tokens]\n",
        "train_y = np.array(train_labels.copy())\n",
        "\n",
        "test_x = [to_numbers(tokens) for tokens in test_tokens]\n",
        "test_y = np.array(test_labels.copy())"
      ],
      "metadata": {
        "id": "d3wmO5IriNQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just to make sure it worked, let's take a look at an example data point from `test_tokens` and `test_x` to see if we properly converted a list of tokens into a list of numbers:"
      ],
      "metadata": {
        "id": "6RQIP3meG5YV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_tokens[0])\n",
        "print(test_x[0])"
      ],
      "metadata": {
        "id": "ClQYlwFvi6f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hopefully it's correct!"
      ],
      "metadata": {
        "id": "-Sd9WV2-HCmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Load Data in Batches"
      ],
      "metadata": {
        "id": "-nyE_dfgjfmM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though an RNN is able to handle input sequences of any length, we still have a small issue: For efficiency purposes, it's *way* faster to train the neural network using data that is split up into \"batches\". (A batch is just a bunch of training examples that are all used at once.) Unfortunately, when training in batches, all the sequences in the batch need to be the same length, so for training purposes we are going to take all of our training data and make the sequences all equal in length (we've chosen to make them 200 tokens long). Sequences that are too long will be truncated (chopped) and sequences that are too short will be padded with the special `\"_PAD\"` token we made earlier.\n",
        "\n",
        "Run the following code to generate the fixed-length sequences:"
      ],
      "metadata": {
        "id": "Iq1TRQJlHN4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad(reviews, length):\n",
        "  result = np.zeros((len(reviews), length), dtype=int)\n",
        "  for i, review in enumerate(reviews):\n",
        "    if len(review) != 0:\n",
        "      result[i, -len(review) :] = np.array(review)[:length]\n",
        "  return np.array(result)\n",
        "\n",
        "seq_len = 200\n",
        "\n",
        "train_x_fixed = pad(train_x, seq_len)\n",
        "test_x_fixed = pad(test_x, seq_len)"
      ],
      "metadata": {
        "id": "rGnbXtDdjkOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check to make sure our data is in a format that makes sense:"
      ],
      "metadata": {
        "id": "MCujblMMISXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x_fixed.shape, train_y.shape)\n",
        "print(test_x_fixed.shape, test_y.shape)"
      ],
      "metadata": {
        "id": "NuBxpbQNkprB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sweet! Now that we have the fixed-length data, we can create some `DataLoader` objects which will help us load our training and testing data in batches efficiently. This code sets up those loaders:"
      ],
      "metadata": {
        "id": "9Y_AQoT7IXIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(train_x_fixed), torch.from_numpy(train_y))\n",
        "\n",
        "test_data = TensorDataset(torch.from_numpy(test_x_fixed), torch.from_numpy(test_y))\n",
        "\n",
        "batch_size = 400\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "zqjKCXGCjpqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Design RNN Model"
      ],
      "metadata": {
        "id": "MVkWI05fk81q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally! Our data is prepared, so we can actually get to work on designing a model. We want to make a recurrent neural network that uses an embedding layer, an RNN layer, a linear layer, and a sigmoid layer. **Set up the model you want to train.** (See the slides for details.)\n",
        "\n",
        "You can refer to the [PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) to read about the parameters the RNN layer takes\n",
        "\n"
      ],
      "metadata": {
        "id": "6SJyCqJCIlP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "input_size = 200\n",
        "embed_dim = 20\n",
        "hidden_size = 30\n",
        "n_layers = 1\n",
        "output_size = 1\n",
        "\n",
        "vocab_size = len(token2num) + 1\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(RNN, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "    self.rnn = nn.RNN(embed_dim, hidden_size, n_layers, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    # ??? TODO: Set up the four different model layers we want\n",
        "    # 1. Encoder layer\n",
        "    # 2. RNN layer\n",
        "    # 3. Linear (\"fully connected\"/MLP) layer\n",
        "    # 4. Sigmoid layer\n",
        "\n",
        "  def forward(self, x):\n",
        "    # ??? TODO: Perform the forward pass\n",
        "    embeds = self.embedding(x.long())\n",
        "    out, hidden = self.rnn(embeds)\n",
        "    last_out = out[:,-1,:]\n",
        "    out = self.fc(last_out)\n",
        "    out = self.sigmoid(out)\n",
        "    return out, hidden"
      ],
      "metadata": {
        "id": "wJe3EvJek-wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Train the RNN Model"
      ],
      "metadata": {
        "id": "Y6sUWdcvlavr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the training data is prepared and the model is designed, we can train the model! There's nothing to do here other than run the code. **Running this will take a few minutes, but you should see the loss (badness score) decreasing over time.** Hopefully Google Colab has assigned you a computer with a GPU. If not, training is going to be *really* slow. You might have to decrease the number of `epochs` of training if that's the case. (You'll get a worse model that way, but at least it won't take as long to train.)"
      ],
      "metadata": {
        "id": "S4y-O9MFI1kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNN()\n",
        "model.to(device)\n",
        "\n",
        "epochs = 20\n",
        "lr = 0.001\n",
        "\n",
        "criterion = nn.BCELoss() \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "model.train()\n",
        "for i in range(epochs):\n",
        "  step = 0\n",
        "\n",
        "  for inputs, labels in train_loader:\n",
        "    inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
        "\n",
        "    model.zero_grad()\n",
        "\n",
        "    output, h = model(inputs)\n",
        "\n",
        "    output = output.squeeze()\n",
        "\n",
        "    loss = criterion(output, labels)\n",
        "    loss.backward()\n",
        "\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "    optimizer.step()\n",
        "\n",
        "    step += 1\n",
        "    if step % 100 == 0:\n",
        "      print(\"loss\", loss)"
      ],
      "metadata": {
        "id": "PDAn_f8rlcN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Use & Test the RNN Model"
      ],
      "metadata": {
        "id": "SCkEcxmS3ruJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6A: Try using the model"
      ],
      "metadata": {
        "id": "lBeVWMEF9WUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Amazing! We've trained a model. Let's give it a try. Run the code below to evaluate the sentence and predict whether it is positive or negative in sentiment. **Then, try changing the sentence to put your model to the test. How well does your model work? Can it handle negating phrases like \"not good\"? Can you fool it with sarcasm?**"
      ],
      "metadata": {
        "id": "eVAu-8BnJXR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"not amazing or good\"\n",
        "\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "x = torch.from_numpy(np.array([to_numbers([token.lower() for token in tokens])])).to(device).float()\n",
        "\n",
        "model.eval()\n",
        "output, hidden = model(x)\n",
        "\n",
        "score = output[0][0]\n",
        "\n",
        "if score >= 0.5:\n",
        "  print(f\"Positive! :) (Score: {score})\")\n",
        "else:\n",
        "  print(f\"Negative! :( (Score: {score})\")"
      ],
      "metadata": {
        "id": "ZZrLttgG3vo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6B: Test the model's accuracy"
      ],
      "metadata": {
        "id": "tRuJM2Rl9YQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can use our testing dataset to get a cold, hard measurement of how well our model works. Give it a try!"
      ],
      "metadata": {
        "id": "9wbPYv45Jqir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "correct_count = 0\n",
        "total_count = 0\n",
        "\n",
        "for inputs, labels in test_loader:\n",
        "  inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
        "\n",
        "  outputs, hiddens = model(inputs)\n",
        "\n",
        "  outputs = outputs.squeeze()\n",
        "\n",
        "  correct = (outputs >= 0.5) == (labels == 1.)\n",
        "  \n",
        "  correct_count += sum(correct)\n",
        "  total_count += correct.shape[0]\n",
        "\n",
        "accuracy = correct_count / total_count\n",
        "\n",
        "print(f\"Testing accuracy: {correct_count}/{total_count} = {int(accuracy * 10000) / 100}%\")"
      ],
      "metadata": {
        "id": "N6voSOri9aiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7 (Bonus): Improve the model"
      ],
      "metadata": {
        "id": "Ggsqjr3PN02w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might be able to improve your accuracy by adjusting your model design or training process (learning rate, epochs, etc).\n",
        "\n",
        "You'll have to go back to steps 4 and 5 to do this. Make sure that if you edit step 4 (the design of the model), you re-train by rerunning step 5. Then test out your new model in step 6!\n",
        "\n",
        "* Consider replacing `nn.RNN()` in the model with `nn.LSTM()` for possibly better results."
      ],
      "metadata": {
        "id": "lcUp9AWLJ7CX"
      }
    }
  ]
}